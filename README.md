# Efficient-Deep-Learning
Related Paper of Efficient Deep Neural Networks

## Reviews and Tutorials
1. [Efficient Processing of Deep Neural Networks: A Tutorial and Survey](https://arxiv.org/abs/1703.09039)
2. [High-Performance Hardware for Machine Learning](https://media.nips.cc/Conferences/2015/tutorialslides/Dally-NIPS-Tutorial-2015.pdf)

## Sparse, Quantization and Compression

### Sparse
1. [Learning both Weights and Connections for Efficient Neural Networks](https://arxiv.org/abs/1506.02626) [NIPS 2015]
2. [Dynamic Network Surgery for Efficient DNNs](https://arxiv.org/abs/1608.04493) [NIPS2016]  [Code](https://github.com/yiwenguo/Dynamic-Network-Surgery)
3. [Learning Structured Sparsity in Deep Neural Networks](https://arxiv.org/abs/1608.03665) [NIPS 2016]  [Code](https://github.com/wenwei202/caffe/tree/scnn)
4. [Sparse Convolutional Neural Networks](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Liu_Sparse_Convolutional_Neural_2015_CVPR_paper.pdf) [CVPR 2015]
5. [Pruning Filters for Efficient ConvNets](https://openreview.net/pdf?id=rJqFGTslg) [ICLR 2017]


### Quantization
1. [Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights](https://arxiv.org/abs/1702.03044) [ICLR 2017]
2. [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/pdf/1510.00149v5.pdf) [ICLR 2016]
3. [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](https://arxiv.org/abs/1603.05279) [ECCV 2016]  [Code](https://github.com/allenai/XNOR-Net)
4. [Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations](https://www.arxiv.org/abs/1609.07061)
5. [Trained Tenary Quantization](https://arxiv.org/abs/1612.01064) [ICLR2017]  [Code](https://github.com/czhu95/ternarynet)
6. [DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients](https://arxiv.org/abs/1606.06160)  [Code](https://github.com/ppwwyyxx/tensorpack/tree/master/examples/DoReFa-Net)
7. [Binaryconnect: Training deep neural networks with binary weights during propagations](https://arxiv.org/pdf/1511.00363.pdf) [NIPS 2015]
8. [Binarize neural networks: Training deep neural networks with weights and activations constrained to +1 or -1](https://arxiv.org/abs/1602.02505) [NIPS 2016]
9. [Googleâ€™s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144)
10. [8-Bit Approximations For Parallelism In Deep Learning](https://arxiv.org/abs/1511.04561) [ICLR 2016]
11. [Quantized Convolutional Neural Networks for Mobile Devices]

## Light Network Structure
1. [SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and < 0.5MB Model Size](https://arxiv.org/pdf/1602.07360v3.pdf)
2. [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861v1) [Code](https://github.com/pby5/MobileNet_Caffe)
3. [PVANet: Lightweight Deep Neural Networks for Real-time Object Detection](https://github.com/sanghoon/pva-faster-rcnn) [Code](https://arxiv.org/abs/1611.08588)
4. 

## Distillation
1. [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) [NIPS 2014]
## Speed Up
1. [Fast Training of Convolutional Networks through FFTs](https://arxiv.org/abs/1312.5851) [ICLR 2013]  [Code](https://github.com/Maratyszcza/NNPACK)
2. [Fast algorithms for convolutional neural networks]() [CVPR 2016]


## Hardware Optimation
1. 
2.
## Related company and product
1.[Movidius](https://www.movidius.com/solutions)

2.[DeePhi Tech](http://www.deephi.com/)

3.[Google TPU]()




4.[Nvidia Tesla V100](https://devblogs.nvidia.com/parallelforall/inside-volta/)

